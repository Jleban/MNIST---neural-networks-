{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to introduce deep neural networks (DNNs) and convolutional neural networks (CNNs) using the high-level Keras package and to become familiar with how to choose its architecture, cost function, and optimizer in Keras. We will also learn how to train neural networks.\n",
    "\n",
    "We will work with the MNIST dataset of hand written digits. The goal is to find a statistical model which recognizes and distinguishes between the ten handwritten digits (0-9).\n",
    "\n",
    "The MNIST dataset comprises handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ nuances of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic classification task. \n",
    "\n",
    "In this Notebook, we show how to use the Keras python package to tackle the MNIST problem with the help of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DNNs with Keras\n",
    "\n",
    "Constructing a Deep Neural Network to solve ML problems is a multiple-stage process. Quite generally, one can identify the key steps as follows:\n",
    "\n",
    "* ***step 1:*** Load and process the data\n",
    "* ***step 2:*** Define the model and its architecture\n",
    "* ***step 3:*** Choose the optimizer and the cost function\n",
    "* ***step 4:*** Train the model \n",
    "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
    "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set\n",
    "\n",
    "We would like to emphasize that, while it is always possible to view steps 1-5 as independent of the particular task we are trying to solve, it is only when they are put together in ***step 6*** that the real gain of using Deep Learning is revealed, compared to less sophisticated methods such as the regression models. With this remark in mind, we shall focus predominantly on steps 1-5 below. We show how one can use grid search methods to find optimal hyperparameters in ***step 6***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Data\n",
    "\n",
    "Keras knows to download automatically the MNIST data from the web. All we need to do is import the `mnist` module and use the `load_data()` class, and it will create the training and test data sets or us.\n",
    "\n",
    "The MNIST set has pre-defined test and training sets, in order to facilitate the comparison of the performance of different models on the data.\n",
    "\n",
    "Once we have loaded the data, we need to format it in the correct shape ($({\\mathrm{N_{samples}}}, {\\mathrm{N_{features}}})$). \n",
    "\n",
    "The size of each sample, i.e. the number of bare features used is N_features (whis is 784 because we have a $28 \\times 28$ pixel grid), while the number of potential classification categories is \"num_classes\" (which is 10, number of digits).\n",
    "\n",
    "Each pixel\n",
    "contains a greyscale value quantified by an integer between\n",
    "0 and 255. To standardize the dataset, we normalize\n",
    "the input data in the interval [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras,sklearn\n",
    "# suppress tensorflow compilation warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "img_rows, img_cols = 28, 28 # number of pixels \n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train[:40000]\n",
    "Y_train = Y_train[:40000]\n",
    "\n",
    "# reshape data, depending on Keras backend\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "    \n",
    "# cast floats to single precesion\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# rescale data in interval [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> First we make a plot of one MNIST digit (2D plot using X data - we reshape it into a $28 \\times 28$ matrix) and label it (which digit does it correspond to?). </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_0 = X_train[0].reshape((28, 28))\n",
    "\n",
    "plt.imshow(X_train_0, cmap=plt.cm.gray)\n",
    "print('label is ', Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we cast the label vectors $y$ to binary class matrices (a.k.a. one-hot format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before conversion - \n",
      "y vector :  [5 0 4 1 9 2 1 3 1 4]\n",
      "after conversion - \n",
      "y vector :  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "\n",
    "print(\"before conversion - \")\n",
    "print(\"y vector : \", Y_train[0:10])\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "print(\"after conversion - \")\n",
    "print(\"y vector : \", Y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this template, we use 40000 training samples and 10000 test samples. Remember that we preprocessed data into the shape $({\\mathrm{N_{samples}}}, {\\mathrm{N_{features}}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 784)\n",
      "Y_train shape: (40000, 10)\n",
      "\n",
      "40000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "We can now move on to construct our deep neural net. We shall use Keras's `Sequential()` class to instantiate a model, and will add different deep layers one by one.\n",
    "\n",
    "Let us create an instance of Keras' `Sequential()` class, called `model`. As the name suggests, this class allows us to build DNNs layer by layer. (https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `add()` method to attach layers to our model. For the purposes of our introductory example, it suffices to focus on `Dense` layers for simplicity. (https://keras.io/layers/core/) Every `Dense()` layer accepts as its first required argument an integer which specifies the number of neurons. The type of activation function for the layer is defined using the `activation` optional argument, the input of which is the name of the activation function in `string` format. Examples include `relu`, `tanh`, `elu`, `sigmoid`, `softmax`.\n",
    "\n",
    "In order for our DNN to work properly, we have to make sure that the numbers of input and output neurons for each layer match. Therefore, we specify the shape of the input in the first layer of the model explicitly using the optional argument `input_shape=(N_features,)`. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. Hence, we only need to specify the size of the softmax output layer to match the number of categories.\n",
    "\n",
    "First, add a `Dense` layer with 400 output neurons and `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another layer with 100 output neurons. Then, we will apply \"dropout,\" a regularization scheme that has been widely adopted in the neural networks literature: during the training procedure neurons\n",
    "are randomly “dropped out” of the neural network with some\n",
    "probability $p$ giving rise to a thinned network. It prevents overfitting by reducing spurious correlations between neurons within the network by introducing\n",
    "a randomization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(100, activation='relu'))\n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to add a soft-max layer since we have a multi-class output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Choose the Optimizer and the Cost Function\n",
    "\n",
    "Next, we choose the loss function according to which to train the DNN. For classification problems, this is the cross entropy, and since the output data was cast in categorical form, we choose the `categorical_crossentropy` defined in Keras' `losses` module. Depending on the problem of interest one can pick any other suitable loss function. To optimize the weights of the net, we choose SGD. This algorithm is already available to use under Keras' `optimizers` module (https://keras.io/optimizers/), but we could use `Adam()` or any other built-in one as well. The parameters for the optimizer, such as `lr` (learning rate) or `momentum` are passed using the corresponding optional arguments of the `SGD()` function. \n",
    "\n",
    "While the loss function and the optimizer are essential for the training procedure, to test the performance of the model one may want to look at a particular `metric` of performance. For instance, in categorical tasks one typically looks at their `accuracy`, which is defined as the percentage of correctly classified data points. \n",
    "\n",
    "To complete the definition of our model, we use the `compile()` method, with optional arguments for the `optimizer`, `loss`, and the validation `metric` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2759: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the model\n",
    "\n",
    "We train our DNN in minibatches. Shuffling the training data during training improves stability of the model. Thus, we train over a number of training epochs. \n",
    "\n",
    "(The number of epochs is the number of complete passes through the training dataset, and the batch size is a number of samples propagated through the network before the model is updated.)\n",
    "\n",
    "Training the DNN is a one-liner using the `fit()` method of the `Sequential` class. The first two required arguments are the training input and output data. As optional arguments, we specify the mini-`batch_size`, the number of training `epochs`, and the test or validation data. To monitor the training procedure for every epoch, we set `verbose=True`. \n",
    "\n",
    "Let us set `batch_size` = 64 and `epochs` = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2294: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "40000/40000 [==============================] - 3s - loss: 1.2014 - acc: 0.6446 - val_loss: 0.5088 - val_acc: 0.8838\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 2s - loss: 0.5894 - acc: 0.8317 - val_loss: 0.3645 - val_acc: 0.9065\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.4755 - acc: 0.8649 - val_loss: 0.3081 - val_acc: 0.9194\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.4101 - acc: 0.8813 - val_loss: 0.2755 - val_acc: 0.9243\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.3716 - acc: 0.8977 - val_loss: 0.2526 - val_acc: 0.9289\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 2s - loss: 0.3445 - acc: 0.9030 - val_loss: 0.2338 - val_acc: 0.9340\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.3185 - acc: 0.9105 - val_loss: 0.2203 - val_acc: 0.9384\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.2991 - acc: 0.9171 - val_loss: 0.2059 - val_acc: 0.9414\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.2815 - acc: 0.9212 - val_loss: 0.1971 - val_acc: 0.9437\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 3s - loss: 0.2656 - acc: 0.9268 - val_loss: 0.1873 - val_acc: 0.9458\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history=model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
    "\n",
    "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9280/10000 [==========================>...] - ETA: 0sTest loss: 0.18727896657288073\n",
      "Test accuracy: 0.9458\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
    "\n",
    "Last, we show how to use the grid search option of scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to optimize the \n",
    "hyperparameters of our model.\n",
    "\n",
    "First, define a function for crating a DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN(optimizer=keras.optimizers.Adam()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With epochs = 1 and batch_size = 64, do grid search over the following optimization schemes: ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 2s - loss: 1.3619 - acc: 0.5840     \n",
      " 8704/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 2s - loss: 1.4104 - acc: 0.5765     \n",
      "10000/10000 [==============================] - 0s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 2s - loss: 1.3410 - acc: 0.6021     \n",
      " 8640/10000 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 2s - loss: 1.3519 - acc: 0.5854     \n",
      " 9472/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4094 - acc: 0.8785     \n",
      " 8576/10000 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4261 - acc: 0.8734     \n",
      "10000/10000 [==============================] - 0s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4136 - acc: 0.8770     \n",
      " 8320/10000 [=======================>......] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4109 - acc: 0.8772     \n",
      " 9536/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.3810 - acc: 0.8871     \n",
      " 9792/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4054 - acc: 0.8800     \n",
      " 9408/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.3897 - acc: 0.8859     \n",
      " 9472/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.3740 - acc: 0.8908     \n",
      " 8896/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.5779 - acc: 0.8325     \n",
      " 9600/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.5903 - acc: 0.8283     \n",
      " 9536/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.5922 - acc: 0.8272     \n",
      " 9792/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.5868 - acc: 0.8263     \n",
      " 9536/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4335 - acc: 0.8714     \n",
      " 9472/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4325 - acc: 0.8720     \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4304 - acc: 0.8721     \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4331 - acc: 0.8738     \n",
      " 8896/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4957 - acc: 0.8503     \n",
      " 8832/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4870 - acc: 0.8600     \n",
      " 8960/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4666 - acc: 0.8621     \n",
      " 9472/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4746 - acc: 0.8600     \n",
      " 8640/10000 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.3560 - acc: 0.8937     \n",
      " 9344/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 5s - loss: 0.3574 - acc: 0.8940     \n",
      " 9216/10000 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 5s - loss: 0.3620 - acc: 0.8925     \n",
      " 9600/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.3661 - acc: 0.8922     \n",
      " 9152/10000 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "40000/40000 [==============================] - 5s - loss: 0.3201 - acc: 0.9064     \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN, \n",
    "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# list of allowed optional arguments for the optimizer, see `compile_model()`\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "# define parameter dictionary\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the mean test score of all optimization schemes and determine which scheme gives the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.951900 using {'optimizer': 'Nadam'}\n",
      "0.850650 (0.013675) with: {'optimizer': 'SGD'}\n",
      "0.946125 (0.001404) with: {'optimizer': 'RMSprop'}\n",
      "0.946725 (0.003144) with: {'optimizer': 'Adagrad'}\n",
      "0.925775 (0.002659) with: {'optimizer': 'Adadelta'}\n",
      "0.946750 (0.001861) with: {'optimizer': 'Adam'}\n",
      "0.935300 (0.002652) with: {'optimizer': 'Adamax'}\n",
      "0.951900 (0.003367) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i>Then we create a DNN with one Dense layer having 200 output neurons. We do the grid search over any 5 different activation functions from https://keras.io/activations/. Let epochs = 1, batches = 64, p_dropout=0.5, and optimizer=keras.optimizers.Adam().  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4175 - acc: 0.8755     \n",
      " 9984/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4101 - acc: 0.8786     \n",
      " 9792/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4028 - acc: 0.8812     \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4110 - acc: 0.8802     \n",
      " 9792/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4314 - acc: 0.8713     \n",
      " 9664/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4294 - acc: 0.8724     \n",
      " 9664/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4327 - acc: 0.8704     \n",
      " 9024/10000 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4223 - acc: 0.8724     \n",
      "10000/10000 [==============================] - 0s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 4s - loss: 0.4344 - acc: 0.8708     \n",
      " 8640/10000 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4264 - acc: 0.8725     \n",
      "10000/10000 [==============================] - 0s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4252 - acc: 0.8733     \n",
      " 8960/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.4263 - acc: 0.8732     \n",
      " 9536/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.6655 - acc: 0.8025     \n",
      " 9152/10000 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.6577 - acc: 0.8015     \n",
      " 9024/10000 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.6558 - acc: 0.8032     \n",
      " 8512/10000 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 0.6577 - acc: 0.8023     \n",
      " 9856/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 1.9461 - acc: 0.4915     \n",
      " 8960/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 1.9551 - acc: 0.4427     \n",
      " 8576/10000 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 1.9426 - acc: 0.4881     \n",
      "10000/10000 [==============================] - 0s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 3s - loss: 1.9461 - acc: 0.5048     \n",
      " 8768/10000 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "40000/40000 [==============================] - 4s - loss: 0.3700 - acc: 0.8905     \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "activation = ['relu', 'tanh', 'elu', 'sigmoid', 'softmax']\n",
    "def create_DNN_2(activation=activation):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation=activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_2, \n",
    "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# define parameter dictionary\n",
    "param_grid = dict(activation=activation)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(X_train,Y_train)   \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.939825 using {'activation': 'relu'}\n",
      "0.939825 (0.002309) with: {'activation': 'relu'}\n",
      "0.915075 (0.006569) with: {'activation': 'tanh'}\n",
      "0.912975 (0.003839) with: {'activation': 'elu'}\n",
      "0.901950 (0.003238) with: {'activation': 'sigmoid'}\n",
      "0.805725 (0.056667) with: {'activation': 'softmax'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> Now, do the grid search over different combination of batch sizes (10, 30, 50, 100) and number of epochs (1, 2, 5). We use the activation function that gave us the highest accuracy.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN_4(activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20,input_shape=(img_rows*img_cols,), activation=activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2759: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2294: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "26666/26666 [==============================] - 5s - loss: 1.0885 - acc: 0.6252     \n",
      "13240/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 1.0112 - acc: 0.6594     \n",
      "13250/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 1.0524 - acc: 0.6327     \n",
      "13110/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 6s - loss: 1.0427 - acc: 0.6350     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 6s - loss: 0.8033 - acc: 0.7200     \n",
      "13150/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 7s - loss: 0.9566 - acc: 0.6793     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 7s - loss: 0.7262 - acc: 0.7580     \n",
      "13040/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 8s - loss: 0.9820 - acc: 0.6577     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 6s - loss: 0.7425 - acc: 0.7398     \n",
      "13070/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 6s - loss: 1.0201 - acc: 0.6517     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 6s - loss: 0.7666 - acc: 0.7409     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 6s - loss: 0.7082 - acc: 0.7579     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.6932 - acc: 0.7652     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.6669 - acc: 0.7738     \n",
      "13100/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 1.0573 - acc: 0.6351     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.7763 - acc: 0.7332     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.7248 - acc: 0.7486     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.7008 - acc: 0.7584     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.6584 - acc: 0.7714     \n",
      "13170/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.9952 - acc: 0.6566     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.7565 - acc: 0.7348     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.7064 - acc: 0.7509     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.6842 - acc: 0.7579     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.6735 - acc: 0.7642     \n",
      "13220/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 1.1108 - acc: 0.6239     \n",
      "12120/13334 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.0903 - acc: 0.6290     \n",
      "12930/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.1248 - acc: 0.6162     \n",
      "13050/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 2s - loss: 1.1318 - acc: 0.6122     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 2s - loss: 0.7706 - acc: 0.7431     \n",
      "12630/13334 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.0979 - acc: 0.6283     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.8041 - acc: 0.7340     \n",
      "12360/13333 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.0983 - acc: 0.6216     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.7754 - acc: 0.7368     \n",
      "12660/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.0990 - acc: 0.6216     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.7776 - acc: 0.7312     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.7108 - acc: 0.7507     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.6849 - acc: 0.7613     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.6619 - acc: 0.7684     \n",
      "12510/13334 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 3s - loss: 1.1177 - acc: 0.6188     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.7793 - acc: 0.7369     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.7115 - acc: 0.7591     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.6784 - acc: 0.7698     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.6590 - acc: 0.7782     \n",
      "12300/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 3s - loss: 1.1299 - acc: 0.6106     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.7844 - acc: 0.7385     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.7181 - acc: 0.7548     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.6949 - acc: 0.7618     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.6615 - acc: 0.7689     \n",
      "13320/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 1.1888 - acc: 0.5924     \n",
      "12000/13334 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.2647 - acc: 0.5690     \n",
      "11750/13333 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.1487 - acc: 0.6080     \n",
      "11650/13333 [=========================>....] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 2s - loss: 1.1654 - acc: 0.6067     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 1s - loss: 0.8040 - acc: 0.7272     \n",
      "13334/13334 [==============================] - 0s     \n",
      "Epoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.1370 - acc: 0.6134     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.7788 - acc: 0.7338     \n",
      "13100/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.2458 - acc: 0.5773     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.8332 - acc: 0.7144     \n",
      "12050/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.3159 - acc: 0.5541     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.8535 - acc: 0.7122     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.7686 - acc: 0.7394     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.7337 - acc: 0.7497     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.7095 - acc: 0.7595     \n",
      "13250/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.2130 - acc: 0.5834     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.8233 - acc: 0.7215     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.7401 - acc: 0.7472     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.7174 - acc: 0.7555     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6933 - acc: 0.7588     \n",
      "12450/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.1664 - acc: 0.6037     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.8070 - acc: 0.7290     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.7311 - acc: 0.7482     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6971 - acc: 0.7587     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6729 - acc: 0.7685     \n",
      "12950/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 1.4479 - acc: 0.4968     \n",
      "12700/13334 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.3387 - acc: 0.5472     \n",
      "13100/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.3805 - acc: 0.5238     \n",
      "12700/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 2s - loss: 1.2862 - acc: 0.5696     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 0s - loss: 0.8834 - acc: 0.7095     \n",
      "13000/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.3864 - acc: 0.5179     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.8875 - acc: 0.7083     \n",
      "12400/13333 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.3541 - acc: 0.5374     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 0s - loss: 0.8903 - acc: 0.7135     \n",
      "12400/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.4029 - acc: 0.5172     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 0s - loss: 0.9081 - acc: 0.6956     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 0s - loss: 0.8189 - acc: 0.7229     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 0s - loss: 0.7604 - acc: 0.7451     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 0s - loss: 0.7284 - acc: 0.7509     \n",
      "13000/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.3399 - acc: 0.5435     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.8651 - acc: 0.7129     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.7904 - acc: 0.7366     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.7430 - acc: 0.7481     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.7086 - acc: 0.7637     \n",
      "12600/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.3300 - acc: 0.5352     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.8727 - acc: 0.7068     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.7711 - acc: 0.7359     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.7259 - acc: 0.7553     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.6968 - acc: 0.7624     \n",
      "12200/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "40000/40000 [==============================] - 13s - loss: 0.9069 - acc: 0.6897    \n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 10s - loss: 0.7031 - acc: 0.7599    \n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 10s - loss: 0.6689 - acc: 0.7727    \n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 10s - loss: 0.6472 - acc: 0.7788    \n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 10s - loss: 0.6359 - acc: 0.7823    \n",
      "Best: 0.914275 using {'activation': 'relu', 'batch_size': 10, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "act = ['relu', 'tanh', 'elu', 'sigmoid', 'softmax']\n",
    "\n",
    "\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_4, verbose=1)\n",
    "\n",
    "    # define parameter dictionary\n",
    "\n",
    "batch_size = [10, 30, 50, 100]\n",
    "epochs = [1, 2, 5]\n",
    "activation = ['relu']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, activation = activation)\n",
    "\n",
    "    # call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "    \n",
    "Score_relu = [grid_result.best_score_, grid_result.best_params_]\n",
    "    \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means_relu = grid_result.cv_results_['mean_test_score']\n",
    "stds_relu = grid_result.cv_results_['std_test_score']\n",
    "params_relu = grid_result.cv_results_['params']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means test score of each case with relu\n",
      "0.889925 (0.004422) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 1}\n",
      "0.903850 (0.002809) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 2}\n",
      "0.914275 (0.001474) with: {'activation': 'relu', 'batch_size': 10, 'epochs': 5}\n",
      "0.886175 (0.006052) with: {'activation': 'relu', 'batch_size': 30, 'epochs': 1}\n",
      "0.903750 (0.002217) with: {'activation': 'relu', 'batch_size': 30, 'epochs': 2}\n",
      "0.913800 (0.002882) with: {'activation': 'relu', 'batch_size': 30, 'epochs': 5}\n",
      "0.876300 (0.008576) with: {'activation': 'relu', 'batch_size': 50, 'epochs': 1}\n",
      "0.897000 (0.006627) with: {'activation': 'relu', 'batch_size': 50, 'epochs': 2}\n",
      "0.908025 (0.002607) with: {'activation': 'relu', 'batch_size': 50, 'epochs': 5}\n",
      "0.864050 (0.005912) with: {'activation': 'relu', 'batch_size': 100, 'epochs': 1}\n",
      "0.889475 (0.002531) with: {'activation': 'relu', 'batch_size': 100, 'epochs': 2}\n",
      "0.908125 (0.000744) with: {'activation': 'relu', 'batch_size': 100, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "print('means test score of each case with relu')\n",
    "for mean, stdev, param in zip(means_relu, stds_relu, params_relu):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best with 'relu': 0.914275 using 0.9142749927327037\n"
     ]
    }
   ],
   "source": [
    "print(\"Best with %r: %f using %s\" % ('relu', Score_relu[0], Score_relu[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "26666/26666 [==============================] - 11s - loss: 0.7821 - acc: 0.7699    \n",
      "13270/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 10s - loss: 0.7523 - acc: 0.7830    \n",
      "13220/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 10s - loss: 0.7649 - acc: 0.7762    \n",
      "13010/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 9s - loss: 0.7577 - acc: 0.7792     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 8s - loss: 0.5311 - acc: 0.8409     \n",
      "13130/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 9s - loss: 0.7438 - acc: 0.7841     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 7s - loss: 0.5241 - acc: 0.8434     \n",
      "13210/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 9s - loss: 0.7671 - acc: 0.7732     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 7s - loss: 0.5431 - acc: 0.8385     \n",
      "13020/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 9s - loss: 0.7497 - acc: 0.7790     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.5347 - acc: 0.8405     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 8s - loss: 0.5010 - acc: 0.8528     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.4849 - acc: 0.8548     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 8s - loss: 0.4785 - acc: 0.8588     \n",
      "13280/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 11s - loss: 0.7449 - acc: 0.7797    \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.5334 - acc: 0.8410     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.5025 - acc: 0.8498     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.4861 - acc: 0.8558     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.4680 - acc: 0.8613     \n",
      "13020/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 8s - loss: 0.7465 - acc: 0.7791     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.5356 - acc: 0.8414     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.4982 - acc: 0.8511     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.4812 - acc: 0.8583     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.4760 - acc: 0.8587     \n",
      "13333/13333 [==============================] - 3s     \n",
      "Epoch 1/1\n",
      "26666/26666 [==============================] - 4s - loss: 0.8856 - acc: 0.7445     \n",
      "12570/13334 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 0.8807 - acc: 0.7500     \n",
      "12330/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 0.8932 - acc: 0.7432     \n",
      "12630/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 4s - loss: 0.8925 - acc: 0.7396     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 2s - loss: 0.5785 - acc: 0.8277     \n",
      "13020/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 0.8699 - acc: 0.7453     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.5638 - acc: 0.8328     \n",
      "12450/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 0.8841 - acc: 0.7434     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.5760 - acc: 0.8330     \n",
      "13170/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 5s - loss: 0.9023 - acc: 0.7378     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.5654 - acc: 0.8346     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.5199 - acc: 0.8462     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.4941 - acc: 0.8536     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.4885 - acc: 0.8537     \n",
      "13050/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.8575 - acc: 0.7513     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.5566 - acc: 0.8387     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5093 - acc: 0.8511     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.4882 - acc: 0.8547     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.4785 - acc: 0.8596     \n",
      "12570/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 5s - loss: 0.8501 - acc: 0.7580     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5569 - acc: 0.8409     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5094 - acc: 0.8515     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.4870 - acc: 0.8574     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.4645 - acc: 0.8601     \n",
      "12570/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 4s - loss: 0.9227 - acc: 0.7352     \n",
      "12300/13334 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 0.9661 - acc: 0.7143     \n",
      "12550/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 0.9547 - acc: 0.7291     \n",
      "12700/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 4s - loss: 0.9806 - acc: 0.7175     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 1s - loss: 0.5951 - acc: 0.8272     \n",
      "13050/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 0.9661 - acc: 0.7220     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.5989 - acc: 0.8247     \n",
      "12200/13333 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 0.9374 - acc: 0.7307     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.5866 - acc: 0.8316     \n",
      "12550/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 4s - loss: 0.9939 - acc: 0.7111     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.6062 - acc: 0.8193     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5349 - acc: 0.8416     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5057 - acc: 0.8496     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.4876 - acc: 0.8539     \n",
      "13000/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 4s - loss: 0.9244 - acc: 0.7333     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5863 - acc: 0.8300     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5290 - acc: 0.8451     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5081 - acc: 0.8518     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.4891 - acc: 0.8581     \n",
      "12500/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 4s - loss: 0.9277 - acc: 0.7356     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5886 - acc: 0.8275     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5278 - acc: 0.8434     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5088 - acc: 0.8467     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.4948 - acc: 0.8545     \n",
      "13050/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 4s - loss: 1.0974 - acc: 0.6804     \n",
      "12500/13334 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 1.0872 - acc: 0.6835     \n",
      "12200/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 5s - loss: 1.0719 - acc: 0.6849     \n",
      "12900/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 4s - loss: 1.0879 - acc: 0.6882     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 1s - loss: 0.6703 - acc: 0.8116     \n",
      "12000/13334 [=========================>....] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 1.0656 - acc: 0.6962     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.6512 - acc: 0.8154     \n",
      "11800/13333 [=========================>....] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 1.1408 - acc: 0.6806     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.6851 - acc: 0.8080     \n",
      "12500/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 4s - loss: 1.1071 - acc: 0.6782     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.6784 - acc: 0.8069     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5818 - acc: 0.8316     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5369 - acc: 0.8432     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5109 - acc: 0.8526     \n",
      "12700/13334 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 4s - loss: 1.1031 - acc: 0.6831     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6521 - acc: 0.8166     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5652 - acc: 0.8364     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5283 - acc: 0.8450     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5072 - acc: 0.8500     \n",
      "13200/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 5s - loss: 1.1064 - acc: 0.6802     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6588 - acc: 0.8168     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5662 - acc: 0.8383     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5210 - acc: 0.8501     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5025 - acc: 0.8536     \n",
      "12500/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "40000/40000 [==============================] - 20s - loss: 0.6829 - acc: 0.7963    \n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 13s - loss: 0.5148 - acc: 0.8466    \n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 13s - loss: 0.4914 - acc: 0.8550    \n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 13s - loss: 0.4767 - acc: 0.8598    \n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 13s - loss: 0.4629 - acc: 0.8620    \n",
      "Best: 0.913950 using {'activation': 'tanh', 'batch_size': 10, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "act = ['relu', 'tanh', 'elu', 'sigmoid', 'softmax']\n",
    "\n",
    "\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_4, verbose=1)\n",
    "\n",
    "    # define parameter dictionary\n",
    "\n",
    "batch_size = [10, 30, 50, 100]\n",
    "epochs = [1, 2, 5]\n",
    "activation = ['tanh']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, activation = activation)\n",
    "\n",
    "    # call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "    \n",
    "Score_tanh = [grid_result.best_score_, grid_result.best_params_]\n",
    "    \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means_tanh = grid_result.cv_results_['mean_test_score']\n",
    "stds_tanh = grid_result.cv_results_['std_test_score']\n",
    "params_tanh = grid_result.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means test score of each case with tanh\n",
      "0.898450 (0.003674) with: {'activation': 'tanh', 'batch_size': 10, 'epochs': 1}\n",
      "0.905675 (0.005771) with: {'activation': 'tanh', 'batch_size': 10, 'epochs': 2}\n",
      "0.913950 (0.003750) with: {'activation': 'tanh', 'batch_size': 10, 'epochs': 5}\n",
      "0.891700 (0.005177) with: {'activation': 'tanh', 'batch_size': 30, 'epochs': 1}\n",
      "0.902550 (0.004015) with: {'activation': 'tanh', 'batch_size': 30, 'epochs': 2}\n",
      "0.911550 (0.003491) with: {'activation': 'tanh', 'batch_size': 30, 'epochs': 5}\n",
      "0.883750 (0.007857) with: {'activation': 'tanh', 'batch_size': 50, 'epochs': 1}\n",
      "0.899200 (0.004641) with: {'activation': 'tanh', 'batch_size': 50, 'epochs': 2}\n",
      "0.909975 (0.001752) with: {'activation': 'tanh', 'batch_size': 50, 'epochs': 5}\n",
      "0.875075 (0.007555) with: {'activation': 'tanh', 'batch_size': 100, 'epochs': 1}\n",
      "0.893025 (0.004453) with: {'activation': 'tanh', 'batch_size': 100, 'epochs': 2}\n",
      "0.906900 (0.002931) with: {'activation': 'tanh', 'batch_size': 100, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "print('means test score of each case with tanh')\n",
    "for mean, stdev, param in zip(means_tanh, stds_tanh, params_tanh):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best with 'tanh': 0.913950 using 0.913949992544949\n"
     ]
    }
   ],
   "source": [
    "print(\"Best with %r: %f using %s\" % ('tanh', Score_tanh[0], Score_tanh[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING  | deprecation.py:506 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING  | deprecation.py:506 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2759: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING  | deprecation.py:323 | From /srv/app/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING  | deprecation.py:506 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2294: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING  | module_wrapper.py:139 | From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "26666/26666 [==============================] - 6s - loss: 0.7739 - acc: 0.7602     \n",
      "13240/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 0.7538 - acc: 0.7726     \n",
      "13270/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 0.7347 - acc: 0.7752     \n",
      "13010/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 6s - loss: 0.7520 - acc: 0.7668     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 6s - loss: 0.5585 - acc: 0.8317     \n",
      "13170/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 8s - loss: 0.7351 - acc: 0.7765     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 6s - loss: 0.5431 - acc: 0.8356     \n",
      "13270/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 8s - loss: 0.7326 - acc: 0.7783     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 7s - loss: 0.5368 - acc: 0.8399     \n",
      "13220/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 8s - loss: 0.7595 - acc: 0.7673     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.5603 - acc: 0.8305     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.5140 - acc: 0.8476     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.4956 - acc: 0.8512     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.4758 - acc: 0.8586     \n",
      "13080/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.7790 - acc: 0.7647     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.5511 - acc: 0.8344     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.5158 - acc: 0.8451     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.5027 - acc: 0.8509     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.4799 - acc: 0.8559     \n",
      "13270/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.7412 - acc: 0.7726     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.5387 - acc: 0.8386     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.5065 - acc: 0.8456     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.4878 - acc: 0.8535     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.4734 - acc: 0.8582     \n",
      "13050/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 0.8635 - acc: 0.7419     \n",
      "12870/13334 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.8443 - acc: 0.7485     \n",
      "13170/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.8665 - acc: 0.7348     \n",
      "12570/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 3s - loss: 0.8626 - acc: 0.7400     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 2s - loss: 0.5687 - acc: 0.8285     \n",
      "12330/13334 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.8312 - acc: 0.7537     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.5694 - acc: 0.8289     \n",
      "12660/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 3s - loss: 0.8454 - acc: 0.7453     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.5760 - acc: 0.8320     \n",
      "12180/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 3s - loss: 0.8802 - acc: 0.7301     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.5831 - acc: 0.8244     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.5361 - acc: 0.8369     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.5029 - acc: 0.8461     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.4874 - acc: 0.8539     \n",
      "12720/13334 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.8740 - acc: 0.7315     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5737 - acc: 0.8280     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5270 - acc: 0.8402     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5032 - acc: 0.8500     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.4890 - acc: 0.8562     \n",
      "12570/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.8548 - acc: 0.7410     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5727 - acc: 0.8286     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5244 - acc: 0.8442     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.5009 - acc: 0.8530     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.4830 - acc: 0.8554     \n",
      "12750/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 0.9430 - acc: 0.7132     \n",
      "12950/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.9784 - acc: 0.7032     \n",
      "12200/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.9124 - acc: 0.7222     \n",
      "12200/13333 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 2s - loss: 0.9362 - acc: 0.7198     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 1s - loss: 0.6060 - acc: 0.8188     \n",
      "12300/13334 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.8952 - acc: 0.7373     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.5820 - acc: 0.8295     \n",
      "13150/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.9483 - acc: 0.7104     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.5844 - acc: 0.8255     \n",
      "12650/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.9891 - acc: 0.7002     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.6163 - acc: 0.8139     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5530 - acc: 0.8350     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5213 - acc: 0.8436     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5056 - acc: 0.8493     \n",
      "11900/13334 [=========================>....] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.9169 - acc: 0.7229     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5921 - acc: 0.8249     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5323 - acc: 0.8430     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5130 - acc: 0.8477     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.4935 - acc: 0.8542     \n",
      "11850/13333 [=========================>....] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.9221 - acc: 0.7263     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5932 - acc: 0.8246     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5319 - acc: 0.8400     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5049 - acc: 0.8494     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.4880 - acc: 0.8552     \n",
      "12750/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 1.1050 - acc: 0.6581     \n",
      "12400/13334 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.0817 - acc: 0.6655     \n",
      "12100/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 1.0603 - acc: 0.6705     \n",
      "11000/13333 [=======================>......] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 2s - loss: 1.0499 - acc: 0.6812     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 0s - loss: 0.6636 - acc: 0.8025     \n",
      "12100/13334 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.0761 - acc: 0.6748     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 0.6597 - acc: 0.8049     \n",
      "10700/13333 [=======================>......] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.0451 - acc: 0.6879     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 0s - loss: 0.6564 - acc: 0.8086     \n",
      "11100/13333 [=======================>......] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.1076 - acc: 0.6640     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.6768 - acc: 0.7972     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.6020 - acc: 0.8204     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5616 - acc: 0.8308     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.5340 - acc: 0.8421     \n",
      "11900/13334 [=========================>....] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.0477 - acc: 0.6801     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6584 - acc: 0.8055     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5920 - acc: 0.8211     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5500 - acc: 0.8356     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5250 - acc: 0.8442     \n",
      "13200/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.0539 - acc: 0.6843     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.6611 - acc: 0.8050     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5783 - acc: 0.8284     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5394 - acc: 0.8390     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.5152 - acc: 0.8475     \n",
      "11500/13333 [========================>.....] - ETA: 0sEpoch 1/5\n",
      "40000/40000 [==============================] - 16s - loss: 0.6907 - acc: 0.7885    \n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 12s - loss: 0.5264 - acc: 0.8407    \n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 11s - loss: 0.5038 - acc: 0.8517    \n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 11s - loss: 0.4856 - acc: 0.8555    \n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 12s - loss: 0.4814 - acc: 0.8578    \n",
      "Best: 0.917650 using {'activation': 'elu', 'batch_size': 10, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "act = ['relu', 'tanh', 'elu', 'sigmoid', 'softmax']\n",
    "\n",
    "\n",
    "\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_4, verbose=1)\n",
    "\n",
    "    # define parameter dictionary\n",
    "\n",
    "batch_size = [10, 30, 50, 100]\n",
    "epochs = [1, 2, 5]\n",
    "activation = ['elu']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, activation = activation)\n",
    "\n",
    "    # call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "    \n",
    "Score_elu = [grid_result.best_score_, grid_result.best_params_]\n",
    "    \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means_elu = grid_result.cv_results_['mean_test_score']\n",
    "stds_elu = grid_result.cv_results_['std_test_score']\n",
    "params_elu = grid_result.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means test score of each case with tanh\n",
      "0.897150 (0.003281) with: {'activation': 'elu', 'batch_size': 10, 'epochs': 1}\n",
      "0.907650 (0.001236) with: {'activation': 'elu', 'batch_size': 10, 'epochs': 2}\n",
      "0.917650 (0.003197) with: {'activation': 'elu', 'batch_size': 10, 'epochs': 5}\n",
      "0.890850 (0.006556) with: {'activation': 'elu', 'batch_size': 30, 'epochs': 1}\n",
      "0.904575 (0.005457) with: {'activation': 'elu', 'batch_size': 30, 'epochs': 2}\n",
      "0.914575 (0.004020) with: {'activation': 'elu', 'batch_size': 30, 'epochs': 5}\n",
      "0.882775 (0.007863) with: {'activation': 'elu', 'batch_size': 50, 'epochs': 1}\n",
      "0.901575 (0.005130) with: {'activation': 'elu', 'batch_size': 50, 'epochs': 2}\n",
      "0.911125 (0.003150) with: {'activation': 'elu', 'batch_size': 50, 'epochs': 5}\n",
      "0.872775 (0.002362) with: {'activation': 'elu', 'batch_size': 100, 'epochs': 1}\n",
      "0.892250 (0.005239) with: {'activation': 'elu', 'batch_size': 100, 'epochs': 2}\n",
      "0.908400 (0.003371) with: {'activation': 'elu', 'batch_size': 100, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "print('means test score of each case with tanh')\n",
    "for mean, stdev, param in zip(means_elu, stds_elu, params_elu):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best with 'elu': 0.917650 using 0.9176499929502606\n"
     ]
    }
   ],
   "source": [
    "print(\"Best with %r: %f using %s\" % ('elu', Score_elu[0], Score_elu[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "26666/26666 [==============================] - 9s - loss: 1.2552 - acc: 0.5928     \n",
      "13300/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 14s - loss: 1.2165 - acc: 0.6100    \n",
      "13160/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 9s - loss: 1.2604 - acc: 0.5913     \n",
      "13200/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 9s - loss: 1.2392 - acc: 0.6005     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 7s - loss: 0.8282 - acc: 0.7274     \n",
      "13150/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 9s - loss: 1.2196 - acc: 0.6076     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 7s - loss: 0.8207 - acc: 0.7292     \n",
      "13050/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 9s - loss: 1.2183 - acc: 0.6069     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 8s - loss: 0.8162 - acc: 0.7321     \n",
      "13030/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 10s - loss: 1.2112 - acc: 0.6101    \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.8157 - acc: 0.7277     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.7304 - acc: 0.7545     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.6954 - acc: 0.7670     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 7s - loss: 0.6736 - acc: 0.7710     \n",
      "13240/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 9s - loss: 1.1960 - acc: 0.6081     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.8285 - acc: 0.7213     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 6s - loss: 0.7526 - acc: 0.7414     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.7177 - acc: 0.7528     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.6894 - acc: 0.7645     \n",
      "13110/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 9s - loss: 1.2184 - acc: 0.6053     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.8302 - acc: 0.7241     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.7476 - acc: 0.7458     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.7121 - acc: 0.7548     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 7s - loss: 0.6884 - acc: 0.7615     \n",
      "13170/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 4s - loss: 1.4387 - acc: 0.5376     \n",
      "13140/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 5s - loss: 1.4588 - acc: 0.5313     \n",
      "12420/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 1.4645 - acc: 0.5305     \n",
      "12480/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 4s - loss: 1.4503 - acc: 0.5266     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 2s - loss: 0.9772 - acc: 0.6776     \n",
      "13334/13334 [==============================] - 2s     \n",
      "Epoch 1/2\n",
      "26667/26667 [==============================] - 5s - loss: 1.4488 - acc: 0.5256     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.9580 - acc: 0.6900     \n",
      "13333/13333 [==============================] - 2s     \n",
      "Epoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 1.4500 - acc: 0.5374     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 0.9581 - acc: 0.6941     \n",
      "13020/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 5s - loss: 1.4869 - acc: 0.5284     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.9796 - acc: 0.6821     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.8415 - acc: 0.7213     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.7866 - acc: 0.7363     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 2s - loss: 0.7544 - acc: 0.7449     \n",
      "13020/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 5s - loss: 1.4514 - acc: 0.5255     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.9578 - acc: 0.6926     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.8281 - acc: 0.7310     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.7694 - acc: 0.7456     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 0.7347 - acc: 0.7565     \n",
      "13260/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 1.4397 - acc: 0.5379     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.9495 - acc: 0.6927     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 3s - loss: 0.8167 - acc: 0.7269     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 5s - loss: 1.5562 - acc: 0.4909     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.0479 - acc: 0.6652     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.9042 - acc: 0.7010     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.8318 - acc: 0.7180     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.7931 - acc: 0.7312     \n",
      "12250/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 5s - loss: 1.5356 - acc: 0.5023     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.0470 - acc: 0.6702     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.9079 - acc: 0.7061     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.8331 - acc: 0.7272     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.7901 - acc: 0.7414     \n",
      "12500/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 4s - loss: 1.7326 - acc: 0.4398     \n",
      "11200/13334 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 3s - loss: 1.7174 - acc: 0.4346     \n",
      "11900/13333 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 4s - loss: 1.7087 - acc: 0.4269     \n",
      "12000/13333 [==========================>...] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 4s - loss: 1.7270 - acc: 0.4344     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 1s - loss: 1.2031 - acc: 0.6257     \n",
      "12800/13334 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 1.7394 - acc: 0.4256     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 1.2055 - acc: 0.6300     \n",
      "12500/13333 [===========================>..] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 4s - loss: 1.7565 - acc: 0.4261     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 0s - loss: 1.2393 - acc: 0.6131     \n",
      "12100/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 4s - loss: 1.7554 - acc: 0.4168     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 1s - loss: 1.2235 - acc: 0.6159     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 0s - loss: 1.0325 - acc: 0.6682     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.9302 - acc: 0.6926     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 0.8695 - acc: 0.7089     \n",
      "13100/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 4s - loss: 1.7411 - acc: 0.4337     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.2026 - acc: 0.6311     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.0007 - acc: 0.6891     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.8988 - acc: 0.7124     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.8380 - acc: 0.7306     \n",
      "12000/13333 [==========================>...] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 4s - loss: 1.7275 - acc: 0.4326     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.2023 - acc: 0.6243     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.0047 - acc: 0.6802     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 0.9008 - acc: 0.7092     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 0s - loss: 0.8406 - acc: 0.7242     \n",
      "11200/13333 [========================>.....] - ETA: 0sEpoch 1/5\n",
      "40000/40000 [==============================] - 14s - loss: 1.1039 - acc: 0.6452    \n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 12s - loss: 0.7564 - acc: 0.7468    \n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 12s - loss: 0.7128 - acc: 0.7616    \n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 12s - loss: 0.6782 - acc: 0.7713    \n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 11s - loss: 0.6620 - acc: 0.7751    \n",
      "Best: 0.909875 using {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "act = ['relu', 'tanh', 'elu', 'sigmoid', 'softmax']\n",
    "\n",
    "\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_4, verbose=1)\n",
    "\n",
    "    # define parameter dictionary\n",
    "\n",
    "batch_size = [10, 30, 50, 100]\n",
    "epochs = [1, 2, 5]\n",
    "activation = ['sigmoid']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, activation = activation)\n",
    "\n",
    "    # call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "    \n",
    "Score_sigmoid = [grid_result.best_score_, grid_result.best_params_]\n",
    "    \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means_sigmoid = grid_result.cv_results_['mean_test_score']\n",
    "stds_sigmoid = grid_result.cv_results_['std_test_score']\n",
    "params_sigmoid = grid_result.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means test score of each case with sigmoid\n",
      "0.881850 (0.007478) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 1}\n",
      "0.894175 (0.004591) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 2}\n",
      "0.909875 (0.007642) with: {'activation': 'sigmoid', 'batch_size': 10, 'epochs': 5}\n",
      "0.857150 (0.007680) with: {'activation': 'sigmoid', 'batch_size': 30, 'epochs': 1}\n",
      "0.881300 (0.002654) with: {'activation': 'sigmoid', 'batch_size': 30, 'epochs': 2}\n",
      "0.902400 (0.002567) with: {'activation': 'sigmoid', 'batch_size': 30, 'epochs': 5}\n",
      "0.832825 (0.013034) with: {'activation': 'sigmoid', 'batch_size': 50, 'epochs': 1}\n",
      "0.877975 (0.004046) with: {'activation': 'sigmoid', 'batch_size': 50, 'epochs': 2}\n",
      "0.894900 (0.002912) with: {'activation': 'sigmoid', 'batch_size': 50, 'epochs': 5}\n",
      "0.803000 (0.016739) with: {'activation': 'sigmoid', 'batch_size': 100, 'epochs': 1}\n",
      "0.860175 (0.005913) with: {'activation': 'sigmoid', 'batch_size': 100, 'epochs': 2}\n",
      "0.891475 (0.004841) with: {'activation': 'sigmoid', 'batch_size': 100, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "print('means test score of each case with sigmoid')\n",
    "for mean, stdev, param in zip(means_sigmoid, stds_sigmoid, params_sigmoid):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best with 'sigmoid': 0.909875 using 0.9098749925777316\n"
     ]
    }
   ],
   "source": [
    "print(\"Best with %r: %f using %s\" % ('sigmoid', Score_sigmoid[0], Score_sigmoid[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "26666/26666 [==============================] - 11s - loss: 1.6375 - acc: 0.4522    \n",
      "13210/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 12s - loss: 1.6825 - acc: 0.4357    \n",
      "13240/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 12s - loss: 1.6866 - acc: 0.4065    \n",
      "13190/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 11s - loss: 1.6689 - acc: 0.4248    \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 7s - loss: 1.3499 - acc: 0.5139     \n",
      "13230/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 11s - loss: 1.6446 - acc: 0.4603    \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 7s - loss: 1.3066 - acc: 0.5342     \n",
      "13230/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 11s - loss: 1.6657 - acc: 0.4452    \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 8s - loss: 1.3579 - acc: 0.4936     \n",
      "13290/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 13s - loss: 1.7527 - acc: 0.3707    \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 9s - loss: 1.4885 - acc: 0.4354     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 9s - loss: 1.3581 - acc: 0.4899     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 9s - loss: 1.2676 - acc: 0.5206     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 8s - loss: 1.1871 - acc: 0.5580     \n",
      "13320/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 12s - loss: 1.6964 - acc: 0.4187    \n",
      "Epoch 2/5\n",
      "22120/26667 [=======================>......] - ETA: 1s - loss: 1.3991 - acc: 0.4898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26667/26667 [==============================] - 7s - loss: 1.7926 - acc: 0.4028     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 3s - loss: 1.5080 - acc: 0.4422     \n",
      "13080/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 7s - loss: 1.8521 - acc: 0.3789     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 2s - loss: 1.5144 - acc: 0.4608     \n",
      "12630/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 7s - loss: 1.8472 - acc: 0.4385     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 3s - loss: 1.4477 - acc: 0.5261     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.3260 - acc: 0.5435     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.2607 - acc: 0.5502     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 2s - loss: 1.2450 - acc: 0.5482     \n",
      "13020/13334 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 7s - loss: 1.8272 - acc: 0.3890     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.5145 - acc: 0.4369     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.4034 - acc: 0.4695     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.3309 - acc: 0.5079     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.2920 - acc: 0.5077     \n",
      "26666/26666 [==============================] - 2s - loss: 1.3421 - acc: 0.4902     \n",
      "12450/13334 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 1.8901 - acc: 0.4301     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 2s - loss: 1.5341 - acc: 0.4945     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.4081 - acc: 0.5011     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.3490 - acc: 0.5058     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.3106 - acc: 0.5095     \n",
      "12450/13333 [===========================>..] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 1.9289 - acc: 0.3827     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.5656 - acc: 0.4716     - ETA\n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.4485 - acc: 0.4860     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.3960 - acc: 0.4982     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.3613 - acc: 0.5024     \n",
      "12700/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 6s - loss: 2.0312 - acc: 0.4016     \n",
      "11900/13334 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 1.9899 - acc: 0.3868     \n",
      "12800/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 5s - loss: 2.0006 - acc: 0.3880     \n",
      "13100/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26666/26666 [==============================] - 6s - loss: 2.0170 - acc: 0.3565     \n",
      "Epoch 2/2\n",
      "26666/26666 [==============================] - 1s - loss: 1.7277 - acc: 0.4206     \n",
      "13200/13334 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 6s - loss: 2.0283 - acc: 0.3670     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 1.7449 - acc: 0.4330     \n",
      "13100/13333 [============================>.] - ETA: 0sEpoch 1/2\n",
      "26667/26667 [==============================] - 6s - loss: 2.0262 - acc: 0.3809     \n",
      "Epoch 2/2\n",
      "26667/26667 [==============================] - 1s - loss: 1.6883 - acc: 0.4870     \n",
      "13300/13333 [============================>.] - ETA: 0sEpoch 1/5\n",
      "26666/26666 [==============================] - 6s - loss: 2.0536 - acc: 0.3966     \n",
      "Epoch 2/5\n",
      "26666/26666 [==============================] - 1s - loss: 1.7485 - acc: 0.4504     \n",
      "Epoch 3/5\n",
      "26666/26666 [==============================] - 1s - loss: 1.5865 - acc: 0.4665     \n",
      "Epoch 4/5\n",
      "26666/26666 [==============================] - 1s - loss: 1.4891 - acc: 0.4902     \n",
      "Epoch 5/5\n",
      "26666/26666 [==============================] - 1s - loss: 1.3952 - acc: 0.5198     \n",
      "12000/13334 [=========================>....] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 2.0185 - acc: 0.3701     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.6913 - acc: 0.4799     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.5115 - acc: 0.5016     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.4166 - acc: 0.5086     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.3506 - acc: 0.5165     \n",
      "11800/13333 [=========================>....] - ETA: 0sEpoch 1/5\n",
      "26667/26667 [==============================] - 6s - loss: 2.0161 - acc: 0.3791     \n",
      "Epoch 2/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.7130 - acc: 0.4375     \n",
      "Epoch 3/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.5614 - acc: 0.4630     \n",
      "Epoch 4/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.4618 - acc: 0.4818     \n",
      "Epoch 5/5\n",
      "26667/26667 [==============================] - 1s - loss: 1.4246 - acc: 0.4827     \n",
      "13333/13333 [==============================] - 3s     \n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 20s - loss: 1.6034 - acc: 0.4298    \n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 14s - loss: 1.2904 - acc: 0.5297    \n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 13s - loss: 1.1338 - acc: 0.5994    \n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 13s - loss: 1.0572 - acc: 0.6244    \n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 13s - loss: 1.0062 - acc: 0.6447    \n",
      "Best: 0.910525 using {'activation': 'softmax', 'batch_size': 10, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "act = ['relu', 'tanh', 'elu', 'sigmoid', 'softmax']\n",
    "\n",
    "\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_4, verbose=1)\n",
    "\n",
    "    # define parameter dictionary\n",
    "\n",
    "batch_size = [10, 30, 50, 100]\n",
    "epochs = [1, 2, 5]\n",
    "activation = ['softmax']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, activation = activation)\n",
    "\n",
    "    # call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "    \n",
    "Score_softmax = [grid_result.best_score_, grid_result.best_params_]\n",
    "    \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means_softmax = grid_result.cv_results_['mean_test_score']\n",
    "stds_softmax = grid_result.cv_results_['std_test_score']\n",
    "params_softmax = grid_result.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means test score of each case with softmax\n",
      "0.820975 (0.035857) with: {'activation': 'softmax', 'batch_size': 10, 'epochs': 1}\n",
      "0.892650 (0.011209) with: {'activation': 'softmax', 'batch_size': 10, 'epochs': 2}\n",
      "0.910525 (0.002899) with: {'activation': 'softmax', 'batch_size': 10, 'epochs': 5}\n",
      "0.759600 (0.028500) with: {'activation': 'softmax', 'batch_size': 30, 'epochs': 1}\n",
      "0.793300 (0.015774) with: {'activation': 'softmax', 'batch_size': 30, 'epochs': 2}\n",
      "0.908700 (0.008550) with: {'activation': 'softmax', 'batch_size': 30, 'epochs': 5}\n",
      "0.729075 (0.059011) with: {'activation': 'softmax', 'batch_size': 50, 'epochs': 1}\n",
      "0.848375 (0.036608) with: {'activation': 'softmax', 'batch_size': 50, 'epochs': 2}\n",
      "0.863975 (0.039225) with: {'activation': 'softmax', 'batch_size': 50, 'epochs': 5}\n",
      "0.761675 (0.016869) with: {'activation': 'softmax', 'batch_size': 100, 'epochs': 1}\n",
      "0.771600 (0.034371) with: {'activation': 'softmax', 'batch_size': 100, 'epochs': 2}\n",
      "0.881350 (0.025424) with: {'activation': 'softmax', 'batch_size': 100, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "print('means test score of each case with softmax')\n",
    "for mean, stdev, param in zip(means_softmax, stds_softmax, params_softmax):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best with 'softmax': 0.910525 using 0.9105249924995005\n"
     ]
    }
   ],
   "source": [
    "print(\"Best with %r: %f using %s\" % ('softmax', Score_softmax[0], Score_softmax[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activation function that gives the best auccracy is  elu with 0.91765\n"
     ]
    }
   ],
   "source": [
    "Best_scores = [['relu',0.914275], ['tanh',0.913950], ['elu', 0.917650], ['sigmoid',0.909875], ['softmax',0.910525]]\n",
    "max = Best_scores[0][1]\n",
    "indice_max_Best_score = 0\n",
    "for i in range(1,len(Best_scores)):\n",
    "    if Best_scores[i][1] > max:\n",
    "        max = Best_scores[i][1]\n",
    "        indice_max_Best_score = i\n",
    "    \n",
    "print('The activation function that gives the best auccracy is ', Best_scores[indice_max_Best_score][0], 'with', Best_scores[indice_max_Best_score][1])\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Then we do the grid search over the number of neurons in the Dense layer and make a plot of mean test score as a function of num_neurons. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2759: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2294: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "26666/26666 [==============================] - 1s - loss: 1.5972 - acc: 0.4337     \n",
      "12032/13334 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 1s - loss: 1.5414 - acc: 0.4237     \n",
      "12800/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 1s - loss: 1.5060 - acc: 0.4266     \n",
      "12032/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 1s - loss: 0.6293 - acc: 0.8031     \n",
      "13312/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 1s - loss: 0.6142 - acc: 0.8079     \n",
      "12480/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 1s - loss: 0.6066 - acc: 0.8110     \n",
      "11776/13333 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 0.5004 - acc: 0.8441     \n",
      "11968/13334 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.4799 - acc: 0.8544     \n",
      "12416/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.4841 - acc: 0.8528     \n",
      "12928/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 2s - loss: 0.3946 - acc: 0.8773     \n",
      "11520/13334 [========================>.....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.3765 - acc: 0.8832     \n",
      "12352/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 2s - loss: 0.3899 - acc: 0.8825     \n",
      "11712/13333 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 3s - loss: 0.3250 - acc: 0.9006     \n",
      "11968/13334 [=========================>....] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 3s - loss: 0.3192 - acc: 0.9035     \n",
      "12672/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 3s - loss: 0.3188 - acc: 0.9027     \n",
      "12608/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 5s - loss: 0.3004 - acc: 0.9068     \n",
      "12096/13334 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 0.3046 - acc: 0.9066     \n",
      "12416/13333 [==========================>...] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 6s - loss: 0.3027 - acc: 0.9091     \n",
      "12480/13333 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "26666/26666 [==============================] - 46s - loss: 0.3589 - acc: 0.9065    \n",
      "13248/13334 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 51s - loss: 0.5187 - acc: 0.8979    \n",
      "13312/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "26667/26667 [==============================] - 43s - loss: 1.0726 - acc: 0.8629    \n",
      "13120/13333 [============================>.] - ETA: 0sEpoch 1/1\n",
      "40000/40000 [==============================] - 7s - loss: 0.2622 - acc: 0.9185     \n"
     ]
    }
   ],
   "source": [
    "def create_DNN_5(optimizer=keras.optimizers.Adam(), num_neurons = 400):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN_5, \n",
    "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# define parameter dictionary\n",
    "num_neurons = [10,50,100,200,500,1000,10000]\n",
    "param_grid = dict(num_neurons=num_neurons)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X_train,Y_train) \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'DNN with Adam optimizer and relu as the activation function')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Mean = []\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean in zip(means):\n",
    "    Mean += [mean]\n",
    "    \n",
    "plt.plot(num_neurons, Mean)\n",
    "plt.xlabel('number of neurons')\n",
    "plt.ylabel('Mean test score ')\n",
    "plt.title('DNN with Adam optimizer and relu as the activation function')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNNs with Keras\n",
    "\n",
    "\n",
    "We have so far considered each MNIST data sample as a $(28\\times 28,)$-long 1d vector. This approach neglects any spatial structure in the image. On the other hand, we do know that in every one of the hand-written digits there are *local* spatial correlations between the pixels, which we would like to take advantage of to improve the accuracy of our classification model. To this end, we first need to reshape the training and test input data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 28, 28, 1)\n",
      "Y_train shape: (40000, 10)\n",
      "\n",
      "40000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# reshape data, depending on Keras backend\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can ask the question of whether a neural net can learn to recognize such local patterns. This can be achieved by using convolutional layers. Luckily, all we need to do is change the architecture of our DNN.\n",
    "\n",
    "\n",
    "After we instantiate the model, add the first convolutional layer with 10 filters, which is the dimensionality of output space. (https://keras.io/layers/convolutional/) Here, we will be concerned with local spatial filters\n",
    "that take as inputs a small spatial patch of the\n",
    "previous layer at all depths. We consider a three-dimensional kernel of size $5\\times5\\times1$. Check out this visualization of the\n",
    "convolution procedure for a square input of unit depth: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "The convolution consists of running this filter over all locations\n",
    "in the spatial plane. After computing the filter, the output is passed through\n",
    "a non-linearity, a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, add a 2D pooling layer. (https://keras.io/layers/pooling/) This pooling layer coarse-grain spatial information by performing\n",
    "a subsampling at each depth. Here, we use the max pool operation. In a max pool, the spatial\n",
    "dimensions are coarse-grained by replacing a small region\n",
    "(say $2\\times2$ neurons) by a single neuron whose output is the\n",
    "maximum value of the output in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3386: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add another convolutional layers with 20 filters and apply dropout. Then, wa add another pooling layer and flatten the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2759: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add second convolutional layer with 20 filters\n",
    "model.add(Conv2D(20, (5, 5), activation='relu'))\n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "# add 2D pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# flatten data\n",
    "model.add(Flatten())\n",
    "# add a dense all-to-all relu layer\n",
    "model.add(Dense(20*4*4, activation='relu'))\n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "# soft-max layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, train your CNN and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2294: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/app/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "40000/40000 [==============================] - 14s - loss: 0.3360 - acc: 0.8945 - val_loss: 0.1114 - val_acc: 0.9784\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.1083 - acc: 0.9664 - val_loss: 0.0599 - val_acc: 0.9854\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0790 - acc: 0.9756 - val_loss: 0.0614 - val_acc: 0.9859\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 14s - loss: 0.0651 - acc: 0.9797 - val_loss: 0.0507 - val_acc: 0.9870\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0569 - acc: 0.9824 - val_loss: 0.0387 - val_acc: 0.9906\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0515 - acc: 0.9845 - val_loss: 0.0350 - val_acc: 0.9899\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0481 - acc: 0.9849 - val_loss: 0.0319 - val_acc: 0.9903\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0435 - acc: 0.9860 - val_loss: 0.0287 - val_acc: 0.9918\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0417 - acc: 0.9868 - val_loss: 0.0307 - val_acc: 0.9915\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 13s - loss: 0.0391 - acc: 0.9874 - val_loss: 0.0273 - val_acc: 0.9921\n",
      " 9664/10000 [===========================>..] - ETA: 0s\n",
      "Test loss: 0.027253641115874052\n",
      "Test accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# train CNN\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> We do the grid search over any 3 different optimization schemes and 2 activation functions. Suppose that we have a 2 convolutional layers with 10 neurons. Let p_dropout = 0.5, epochs = 1, and batch_size = 64. We then determine which combination of optimization scheme, activation function, and number of neurons gives the best accuracy. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 7s - loss: 1.1450 - acc: 0.6280     \n",
      " 9984/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 1.3913 - acc: 0.5411     \n",
      "10000/10000 [==============================] - 1s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 1.2757 - acc: 0.5739     \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 7s - loss: 1.2027 - acc: 0.6054     \n",
      " 9600/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 0.6027 - acc: 0.8077     \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 0.5959 - acc: 0.8104     \n",
      " 9984/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 10s - loss: 0.6220 - acc: 0.7986    \n",
      " 9472/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 0.5610 - acc: 0.8221     \n",
      "10000/10000 [==============================] - 2s     \n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 0.6942 - acc: 0.7753     \n",
      " 9664/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 0.7414 - acc: 0.7585     \n",
      " 9408/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 0.7196 - acc: 0.7741     \n",
      " 9536/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 8s - loss: 0.7333 - acc: 0.7634     \n",
      " 9856/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 1.2508 - acc: 0.5930     \n",
      " 9664/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 10s - loss: 1.1221 - acc: 0.6319    \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 1.1447 - acc: 0.6256     \n",
      " 9472/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 1.1448 - acc: 0.6230     \n",
      " 9792/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 0.6497 - acc: 0.7922     \n",
      " 9664/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 0.5935 - acc: 0.8117     \n",
      " 9600/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 0.6955 - acc: 0.7725     \n",
      " 9856/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 0.6130 - acc: 0.8011     \n",
      " 9920/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 9s - loss: 0.6396 - acc: 0.7973     \n",
      " 9728/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 10s - loss: 0.7201 - acc: 0.7668    \n",
      " 9728/10000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 11s - loss: 0.6708 - acc: 0.7841    \n",
      " 9664/10000 [===========================>..] - ETA: 0sEpoch 1/1\n",
      "30000/30000 [==============================] - 10s - loss: 0.7196 - acc: 0.7693    \n",
      "10000/10000 [==============================] - 2s     \n",
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 12s - loss: 0.5395 - acc: 0.8297    \n",
      "Best: 0.949825 using {'activation': 'relu', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "def DNN(activation='relu', optimizer='Adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, (5, 5), activation=activation, input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(10, (5, 5), activation=activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    # add 2D pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # flatten data\n",
    "    model.add(Flatten())\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "model_gridsearch = KerasClassifier(build_fn=DNN, \n",
    "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# define parameter dictionary\n",
    "optimizer = ['SGD', 'Adam', 'Adamax']\n",
    "activation = ['relu', 'elu']\n",
    "param_grid = dict(optimizer=optimizer, activation=activation)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(X_train,Y_train)   \n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.949825 using {'activation': 'relu', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> Then, we create an arbitrary DNN (you are free to choose any activation function, optimization scheme, etc) and evaluate its performance. Then, we add two convolutional layers and pooling layers and evaluate its performance again. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 2s - loss: 1.4777 - acc: 0.4511 - val_loss: 0.7666 - val_acc: 0.8537\n",
      " 8864/10000 [=========================>....] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "def create_DNN_6():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    return model\n",
    "\n",
    "model = create_DNN_6()\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "# train CNN\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score_1 = model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 28, 28, 1)\n",
      "Y_train shape: (40000, 10)\n",
      "\n",
      "40000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# reshape data, depending on Keras backend\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 12s - loss: 1.4160 - acc: 0.4639 - val_loss: 0.6298 - val_acc: 0.9225\n",
      " 9824/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, (5, 5), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(10, (5, 5), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# add 2D pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# flatten data\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# add a dense all-to-all relu layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "score_2 = model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss without conventional layers: 0.7665949113845825\n",
      "Test accuracy without conventional layers: 0.8537\n",
      "Test loss with conventional layers: 0.629766546344757\n",
      "Test accuracy with conventional layers: 0.9225\n"
     ]
    }
   ],
   "source": [
    "print('Test loss without conventional layers:', score_1[0])\n",
    "print('Test accuracy without conventional layers:', score_1[1])\n",
    "print('Test loss with conventional layers:', score_2[0])\n",
    "print('Test accuracy with conventional layers:', score_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding conventional layers reduces the loss and increases the accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
